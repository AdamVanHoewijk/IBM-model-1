{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "stylish-haiti",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "humanitarian-lincoln",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the sentences\n",
    "with open(r'data/europarl-v7.sv-en.lc.sv') as f:\n",
    "    sv_sentences = [line.rstrip('\\n') for line in f]\n",
    "sv_sentences = [i[:-2] for i in sv_sentences]\n",
    "\n",
    "with open(r'data/europarl-v7.sv-en.lc.en') as f:\n",
    "    en_sentences = [line.rstrip('\\n') for line in f]\n",
    "en_sentences = [i[:-2] for i in en_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "vertical-municipality",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all the words\n",
    "sv_words = [word for sentence in sv_sentences for word in sentence.split()]\n",
    "en_words = [word for sentence in en_sentences for word in sentence.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "historic-forge",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a counter to count all the words\n",
    "sv_counter = collections.Counter(sv_words)\n",
    "en_counter = collections.Counter(en_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "moving-volunteer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('att', 9181),\n",
       " (',', 8876),\n",
       " ('och', 7038),\n",
       " ('i', 5949),\n",
       " ('det', 5687),\n",
       " ('som', 5028),\n",
       " ('fÃ¶r', 4959),\n",
       " ('av', 4013),\n",
       " ('Ã¤r', 3840),\n",
       " ('en', 3724)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sv_counter.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "plastic-hygiene",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 19322),\n",
       " (',', 13513),\n",
       " ('of', 9312),\n",
       " ('to', 8801),\n",
       " ('and', 6946),\n",
       " ('in', 6090),\n",
       " ('is', 4400),\n",
       " ('that', 4357),\n",
       " ('a', 4269),\n",
       " ('we', 3223)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_counter.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "noticed-creature",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of speaker appearing 3.682725806332815e-05\n",
      "Probability of zebra appearing 0.0\n"
     ]
    }
   ],
   "source": [
    "total_occurence = sum(en_counter.values())\n",
    "\n",
    "print('Probability of speaker appearing', en_counter['speaker']/total_occurence)\n",
    "print('Probability of zebra appearing', en_counter['zebra']/total_occurence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "tough-reliance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counts occurance of two words next to eachother\n",
    "def occurance(words, next_word, previous_word=None):\n",
    "    count = 0\n",
    "    t = False\n",
    "    if (previous_word == None):\n",
    "        count = collections.Counter(words)[next_word]\n",
    "    \n",
    "    for word in words:\n",
    "        if (word == next_word and t):\n",
    "            count += 1\n",
    "        if (word == previous_word):\n",
    "            t = True\n",
    "        else:\n",
    "            t = False\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "heard-motel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on words being next to each other, calculate the probability of a sentence\n",
    "def sentence_proba(words,str):\n",
    "    counter = collections.Counter(words)\n",
    "    probabilities = []\n",
    "    previous_word = None\n",
    "    \n",
    "    # Iterate over words\n",
    "    for next_word in str.split():\n",
    "        numb_of_occ = occurance(words, next_word, previous_word)\n",
    "        \n",
    "        # if it's the first word, calc the chance of just that word\n",
    "        if (previous_word == None):\n",
    "            probabilities.append((numb_of_occ + 1)/sum(counter.values()))\n",
    "            \n",
    "        # Else calculate the chance of those words being next to one another\n",
    "        else:\n",
    "            probabilities.append((numb_of_occ + 1)/counter[previous_word])\n",
    "        previous_word = next_word\n",
    "        \n",
    "    return np.prod(probabilities) * len(probabilities)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "vital-employee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7791098071080344e-07"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_proba(en_words, 'i have received your attention')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sudden-loading",
   "metadata": {},
   "source": [
    "If a word or combination of two words never occur in training, probability becomes 0\n",
    "\n",
    "if its a long sentese probabiliy goes towards 0 \n",
    "\n",
    "to avoid getting probability of 0 on a sentece, we add a seudo count of 1 all occurances, if something appears 0 times we now say it apreas 1 time so probability does not go to 0, if it apear 5000 times we now say it appear 5001 times\n",
    "\n",
    "to avoid long senteses getting to small chance, we devide on number of words in sentes to get avrage probability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coated-awareness",
   "metadata": {},
   "source": [
    "# C\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "jewish-planet",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set dictionary\n",
    "sv_dict = list(set(sv_words))\n",
    "en_dict = list(set(en_words))+['NULL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "personalized-richmond",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save indexes for speed\n",
    "sv_dict_indexes = dict(zip(sv_dict,range(len(sv_dict))))\n",
    "en_dict_indexes = dict(zip(en_dict,range(len(en_dict))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "anticipated-confidentiality",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = pd.DataFrame(np.random.rand(len(sv_dict), len(en_dict)),columns=en_dict, index=sv_dict)\n",
    "t = copy.deepcopy(t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "authorized-pharmaceutical",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "# Run EM algorithm for iterations.\n",
    "for j in range(9):\n",
    "    \n",
    "    # Reset the counter for every run\n",
    "    Ces = pd.DataFrame(np.zeros((len(sv_dict), len(en_dict))),columns=en_dict, index=sv_dict)\n",
    "    Ce  = {e:0 for e in en_dict}\n",
    "    \n",
    "    # For every sentence..\n",
    "    for i in range(len(sv_sentences)): \n",
    "        current_en_sentence = en_sentences[i].split() + ['NULL']\n",
    "        \n",
    "        # Loop through every swedish word..\n",
    "        for sv_word in sv_sentences[i].split():\n",
    "            sv_index = sv_dict_indexes[sv_word]\n",
    "            t_total = t.loc[sv_word, current_en_sentence].sum()\n",
    "            \n",
    "            # Pair every swedish word up with an english word..\n",
    "            for en_word in current_en_sentence:\n",
    "                en_index = en_dict_indexes[en_word]\n",
    "                \n",
    "                # And calculate the counts according to the IMB Model 1\n",
    "                delta = (t.values[sv_index, en_index] / t_total)\n",
    "                Ces.values[sv_index, en_index] = Ces.values[sv_index, en_index] + delta\n",
    "                Ce[en_word] = Ce.get(en_word) + 1\n",
    "    \n",
    "    # Update the t matrix with the new values.\n",
    "    for en_word in en_dict:   \n",
    "        en_index = en_dict_indexes[en_word]\n",
    "        t.values[:,en_index] = Ces.values[:,en_index]/Ce.get(en_word)\n",
    "        \n",
    "    # Keep track of iterations to compare.\n",
    "    t_list.append(copy.deepcopy(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "possible-athletics",
   "metadata": {},
   "source": [
    "### Translating\n",
    "The model is trained, save it as pickle file so we can use it in the future!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divided-genome",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the t matrix to use in future.\n",
    "t.to_pikle('t_e_s.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "global-civilization",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_used = pd.read_pickle('t_e_s.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "pediatric-dynamics",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "europeiska    0.009607\n",
       "i             0.000622\n",
       "att           0.000286\n",
       "och           0.000134\n",
       "den           0.000125\n",
       "som           0.000080\n",
       "en            0.000070\n",
       "fÃ¶r          0.000059\n",
       "europeisk     0.000044\n",
       "till          0.000039\n",
       "Name: european, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the words most probable to be transalte from \"european\"\n",
    "t_used['european'].sort_values(ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retired-revision",
   "metadata": {},
   "source": [
    "Dictionary gets shuffled so lets update it to the matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "material-recorder",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the dictionaries and indexes to match the current t matrix used.\n",
    "en_dict = t_used.columns\n",
    "sv_dict = list(t_used.index.values)\n",
    "\n",
    "sv_dict_indexes = dict(zip(sv_dict,range(len(sv_dict))))\n",
    "en_dict_indexes = dict(zip(en_dict,range(len(en_dict))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "authorized-seeker",
   "metadata": {},
   "source": [
    "We use bayes furmula to get values proportional to the probability by multiplying the rate of which a word occus by the transition probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "statistical-average",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our probability matrix which is p(e|s)=p(e)*t(s|e)\n",
    "q = copy.deepcopy(t_used)\n",
    "counter = collections.Counter(en_words)\n",
    "total = sum(counter.values())\n",
    "\n",
    "# For every word in the dictionary\n",
    "for en_word in en_dict: \n",
    "        en_index = en_dict_indexes[en_word]\n",
    "        \n",
    "        # p(e|s)=p(e)*t(s|e)\n",
    "        q.values[:,en_index] = t_used.values[:,en_index]*(counter[en_word]/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "color-navigator",
   "metadata": {},
   "outputs": [],
   "source": [
    "q.drop(',', axis=1, inplace=True)\n",
    "q.drop(',', axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "restricted-logic",
   "metadata": {},
   "source": [
    "Let's test a swedish word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "legal-means",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "able           4.878776e-06\n",
       "could          1.889025e-06\n",
       "wound          4.602928e-07\n",
       "enhancement    4.506636e-07\n",
       "join           3.504941e-07\n",
       "faster         2.991363e-07\n",
       "preclude       2.985990e-07\n",
       "heal           2.945410e-07\n",
       "medal          2.630517e-07\n",
       "schedule       2.539770e-07\n",
       "Name: kunna, dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.loc['kunna'].sort_values(ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bridal-bailey",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from itertools import chain\n",
    "\n",
    "# Translate a sentence, given all the words used.\n",
    "def translate(sentence,words):\n",
    "    \n",
    "    # Find the best words based on our q matrix\n",
    "    best_words = []\n",
    "    for word in sentence.split(): \n",
    "        best_words.append(q.loc[word,:].sort_values(ascending=False).index[0])\n",
    "    \n",
    "    while 'NULL' in best_words:\n",
    "        best_words.remove('NULL')\n",
    "    \n",
    "    # Iterate over all perutations and find the most probable one.\n",
    "    permutations = itertools.permutations(best_words)\n",
    "    max_proba = 0\n",
    "    best_translation = None\n",
    "    for permutation in permutations:\n",
    "        permutation = ' '.join(word for word in permutation)\n",
    "        sp = sentence_proba(words,permutation)\n",
    "        \n",
    "        # Save best translation\n",
    "        if sp > max_proba:\n",
    "            max_proba = sp\n",
    "            best_translation = permutation\n",
    "            \n",
    "    return best_translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "metropolitan-boating",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "despite relative stability\n",
      "now interpretative there is\n",
      "not accountable stillborn they have\n"
     ]
    }
   ],
   "source": [
    "print(translate('trots relativ stabilitet', en_words))\n",
    "print(translate('det finns nu visserligen', en_words))\n",
    "print(translate('de ansvariga har inte tagit', en_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "irish-volleyball",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
